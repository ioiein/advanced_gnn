{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab_scalable_gnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZT4qfpC0Sja"
      },
      "source": [
        "# Lab â€” Scalable GNN\n",
        "\n",
        "Today we will talk about application of Graph Neural Networks to large graphs, specifically, we will explain in details how GraphSAINT (Graph Sampling based Inductive Learning Method https://arxiv.org/abs/1907.04931) works.\n",
        "\n",
        "Classic GCN samplings works in layer-wise nature. For example, GraphSAGE samples the neighborhood of each node to aggregate into the anchor node embedding. The general idea of GraphSAINT is to sample subgraphs from the large graph and train the GCN on it. However, such sampling induces bias due to more probable sampling of high degree nodes. To eliminate it, authors propose several normalization and variance-reduction techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHex3aef0PL1"
      },
      "source": [
        "! pip install ogb -q\n",
        "! pip install dgl-cu111 -f https://data.dgl.ai/wheels/repo.html -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIy_54Ea23KK"
      },
      "source": [
        "import dgl\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from abc import ABC, abstractmethod\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import dgl.function as fn\n",
        "from dgl.sampling import random_walk, pack_traces\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV5ovDYLFFXB"
      },
      "source": [
        "from ogb.nodeproppred import DglNodePropPredDataset, Evaluator\n",
        "\n",
        "d_name = \"ogbn-arxiv\"\n",
        "\n",
        "dataset = DglNodePropPredDataset(name=d_name)\n",
        "\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "graph, label = dataset[0]\n",
        "\n",
        "graph.ndata[\"label\"] = label.view(-1)\n",
        "graph.ndata[\"is_train\"] = torch.zeros(graph.num_nodes())\n",
        "graph.ndata[\"is_train\"][train_idx] = 1\n",
        "graph.ndata[\"is_train\"] = graph.ndata[\"is_train\"].type(torch.BoolTensor)\n",
        "\n",
        "graph = graph.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJBRNRanbPnv"
      },
      "source": [
        "### Subgraph sampling\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/netspractice/advanced_gnn/main/lab_scalable_gnn/graph_sampling.png' width=700>\n",
        "\n",
        "In this task we will define the abstract SamplerBase class and specific realizations of it for different motifs sampling (Node, Edge and Random Walks).\n",
        "\n",
        "The `SamplerBase` inherit the `torch.utils.data.Dataset`. `SamplerBase` takes two input parameters:\n",
        "\n",
        "1. Graph `g` is our graph to be sampled\n",
        "2. `num_nodes` is an expected number of nodes for each subgraph\n",
        "\n",
        "The torch Dataset requires at least two functions to be defined: `__len__` and `__getitem__`.\n",
        "\n",
        "First function should return the total number of samples in the dataset. In the given setting we want to cover the all nodes by subgraphs, so the length of dataset will be equal to the ceil of number of nodes in the graph divided by the number of nodes per subgraph.\n",
        "\n",
        "The second one should return the sample subgraph. To unify it between different sampling techniques (Nodes, Edges and Random Walks), we will call abstract method `__sample__` that returns list of nodes for subgraph. After, we will take the subgraph from the given nodes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCFrttNgcUKJ"
      },
      "source": [
        "class SamplerBase(Dataset, ABC):\n",
        "  def __init__(self, g, num_nodes, seed=0):\n",
        "    self.g = g\n",
        "    self.num_nodes = num_nodes\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "  @abstractmethod\n",
        "  def __sample__(self):\n",
        "    raise NotImplemented\n",
        "  \n",
        "  def __len__(self):\n",
        "    return math.ceil(self.g.num_nodes() / self.num_nodes)\n",
        "\n",
        "  @abstractmethod\n",
        "  def __getitem__(self, idx):\n",
        "    raise NotImplemented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lHj5oyC-BWC"
      },
      "source": [
        "After defining the base for samplers we can define the specific realizations.\n",
        "\n",
        "The `NodeSampler` just samples with replacement nodes from the graphs according to its input degree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58m8EgFS-LgH"
      },
      "source": [
        "class NodeSampler(SamplerBase):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    self.prob = None\n",
        "    super(NodeSampler, self).__init__(*args, **kwargs)\n",
        "\n",
        "  def __sample__(self):\n",
        "    if self.prob is None:\n",
        "      self.prob = self.g.in_degrees().float().clamp(min=1).to(device)\n",
        "    return torch.multinomial(self.prob, num_samples=self.num_nodes, replacement=True).unique()\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    subgraph_nodes = self.__sample__()\n",
        "    return dgl.node_subgraph(self.g, subgraph_nodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35fnW5H0E_sC"
      },
      "source": [
        "node_sampler = NodeSampler(graph, 100)\n",
        "\n",
        "subg = node_sampler.__getitem__(0)\n",
        "assert subg.num_nodes() > 90\n",
        "assert subg.num_nodes() < 110"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmn1k8v4-0wB"
      },
      "source": [
        "`EdgeSampler` works as follows:\n",
        "\n",
        "1. Samples edges with _unnormalized_ probability \n",
        "$$p_{u,v} = \\frac{1}{\\text{deg}(u)} + \\frac{1}{\\text{deg}(v)}$$\n",
        "2. Returns unique list of nodes defined by the edges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzlw1_ms-LcO"
      },
      "source": [
        "from time import time\n",
        "\n",
        "class EdgeSampler(SamplerBase):\n",
        "  def __init__(self, num_edges, **kwargs):\n",
        "    self.prob = None\n",
        "    self.num_edges = num_edges\n",
        "    super(EdgeSampler, self).__init__(num_nodes=2 * num_edges, **kwargs)\n",
        "\n",
        "  def __sample__(self):\n",
        "    src, dst = self.g.edges()\n",
        "    if self.prob is None:\n",
        "      src_degrees = self.g.in_degrees(src).float().clamp(min=1)\n",
        "      dst_degrees = self.g.in_degrees(dst).float().clamp(min=1)\n",
        "\n",
        "      self.prob = (1. / src_degrees + 1. / dst_degrees).to(device)\n",
        "      self.prob /= self.prob.sum()\n",
        "    return torch.multinomial(self.prob, self.num_edges, replacement=False)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    subgraph_edges = self.__sample__()\n",
        "    return dgl.edge_subgraph(self.g, subgraph_edges)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZr1s2PmHeQZ"
      },
      "source": [
        "edge_sampler = EdgeSampler(num_edges=256, g=graph)\n",
        "\n",
        "subg = edge_sampler.__getitem__(0)\n",
        "assert subg.num_edges() == 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prpvCDBU-6vs"
      },
      "source": [
        "`RandomWalkSampler` works as follows:\n",
        "\n",
        "1. Selects the random root nodes from graph\n",
        "2. Uses `random_walk` method from dgl to sample random walks\n",
        "3. Packs random walk traces with `pack_traces` from dgl\n",
        "4. Returns the received node list from `3`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkfpyznr-LaS"
      },
      "source": [
        "class RandomWalkSampler(SamplerBase):\n",
        "  def __init__(self, num_roots, length, **kwargs):\n",
        "    self.num_roots, self.length = num_roots, length\n",
        "    super(RandomWalkSampler, self).__init__(num_nodes=num_roots * length, **kwargs)\n",
        "  \n",
        "  def __sample__(self):\n",
        "    sampled_roots = torch.randint(0, self.g.num_nodes(), (self.num_roots,))\n",
        "    traces, types = random_walk(self.g, nodes=sampled_roots, length=self.length)\n",
        "    sampled_nodes, _, _, _ = pack_traces(traces, types)\n",
        "    return  sampled_nodes.unique()\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    subgraph_nodes = self.__sample__()\n",
        "    return dgl.node_subgraph(self.g, subgraph_nodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhJkg8zK-LXP"
      },
      "source": [
        "rw_sampler = RandomWalkSampler(num_roots=20, length=10, g=graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2ATrNdbG0uH"
      },
      "source": [
        "### Precompute normalizations\n",
        "\n",
        "Subgraph sampling induce the bias towards nodes with high degrees due to larger probability to be sampled. So, authors of GraphSAINT propose to use the normalizations. To decrease train time, they fix normalization before starting training by precomputing it on the several samples.\n",
        "\n",
        "To do so, we need to sample several batches of subgraphs and calculate edge, loss and aggregation norms.\n",
        "\n",
        "To simplify our code, we will use custom collate function that will extract node and edge indices and calculate summed number of nodes in the sampled subgraphs.\n",
        "\n",
        "Method `collate_batch` should iterate over each subgraph in the batch, and calculate three variables:\n",
        "\n",
        "1. Number of nodes over all graphs in the batch (not unique, total)\n",
        "2. List of lists with source node ids (`dgl.NID` param of `graph.ndata`)\n",
        "3. List of lists with source edge ids (`dgl.EID` param of `graph.edata`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOA9rzWBIa5s"
      },
      "source": [
        "def collate_batch(batch):\n",
        "  sum_num_nodes = 0\n",
        "  subgraphs_nids_list = []\n",
        "  subgraphs_eids_list = []\n",
        "  for subg in batch:\n",
        "      sum_num_nodes += subg.num_nodes()\n",
        "      subgraphs_nids_list.append(subg.ndata[dgl.NID])\n",
        "      subgraphs_eids_list.append(subg.edata[dgl.EID])\n",
        "  return sum_num_nodes, subgraphs_nids_list, subgraphs_eids_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R_TXL6kQE9Q"
      },
      "source": [
        "batch = [graph.subgraph(range(i * 100, (i + 1) * 100)) for i in range(10)]\n",
        "sum_num_nodes, subgraphs_nids_list, subgraphs_eids_list = collate_batch(batch)\n",
        "\n",
        "assert sum_num_nodes == 1000\n",
        "assert len(subgraphs_nids_list) == 10\n",
        "assert len(subgraphs_eids_list) == 10\n",
        "assert min(subgraphs_nids_list[3]) == 300\n",
        "assert max(subgraphs_nids_list[3]) == 399"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0geRyaMpIfDz"
      },
      "source": [
        "Now, we need to define method to compute _aggregator_ and _loss normalizations_, given the node and edge occurencies. The loss normalization is defined as follows:\n",
        "\n",
        "$$\\lambda_{i} = \\frac{N_\\text{samples}}{N_{i} \\cdot N_\\text{nodes}},$$\n",
        "\n",
        "where $N_\\text{samples}$ is a number of subgraphs in the precomputing stage, $N_{i}$ is a number of occurencies of node $i$ in subgraphs and $N_\\text{nodes}$ is a total number of nodes in the graph. The aggregator normalization is defined as follows:\n",
        "\n",
        "$$\\alpha_{u,v} = \\frac{p_{u,v}}{p_v},$$\n",
        "\n",
        "where $p_{u,v}$ is a probability of edge from node $u$ to node $v$ and $p_v$ is a probability of node $v$. To calculate it, one can use dgl message function `fn.e_div_v`.\n",
        "\n",
        "\n",
        "$\\alpha$ will be used as an edge weight in the `GraphConv` layer further. The $\\lambda$ will be used as a observation weight while cross-entropy calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VveFUEzgIxKu"
      },
      "source": [
        "def compute_norm(g, node_counter, edge_counter, num_subgraphs):\n",
        "  node_counter[node_counter == 0] = 1\n",
        "  edge_counter[edge_counter == 0] = 1\n",
        "\n",
        "  loss_norm = num_subgraphs / node_counter / g.num_nodes()\n",
        "  g.ndata['n_c'] = node_counter\n",
        "  g.edata['e_c'] = edge_counter\n",
        "  g.apply_edges(fn.e_div_v('e_c', 'n_c', 'a_n'))\n",
        "  aggr_norm = g.edata.pop('a_n')\n",
        "\n",
        "  g.ndata.pop('n_c'), g.edata.pop('e_c')\n",
        "\n",
        "  return aggr_norm, loss_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x73RaNJxPLqt"
      },
      "source": [
        "g = dgl.rand_graph(10, 30)\n",
        "aggr, loss = compute_norm(\n",
        "    g, \n",
        "    torch.arange(10).type(torch.FloatTensor), \n",
        "    torch.arange(30).type(torch.FloatTensor), \n",
        "    100\n",
        ")\n",
        "assert round(loss[-1].item(), 2) == 1.11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP72mnplLvJU"
      },
      "source": [
        "Now we are ready to write function for precomputation stage.\n",
        "\n",
        "It should work as follows:\n",
        "\n",
        "1. Initialize DataLoader with input sampler (do not forget to pass the `collate_fn=collate_batch` parameter)\n",
        "2. Iterate until data left or until we receive `num_samples` for each node (break iteration if `num sampled nodes > num_samples * sampler.g.num_nodes`)\n",
        "3. On each iteration calculate\n",
        "  1. total number of sampled nodes \n",
        "  2. total number of subgraphs (`num_subgraphs`)\n",
        "  3. counters for each node (`node_counter`) and edge occurencies (`edge_counter`)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVmV0mwB2wd0"
      },
      "source": [
        "def precompute_norm(sampler, num_samples, batch_size, num_workers):\n",
        "  loader = DataLoader(\n",
        "      sampler, \n",
        "      batch_size=batch_size, \n",
        "      shuffle=True, \n",
        "      num_workers=num_workers, \n",
        "      collate_fn=collate_batch, \n",
        "      drop_last=False)\n",
        "  node_counter = torch.zeros((sampler.g.num_nodes(),)).to(device)\n",
        "  edge_counter = torch.zeros((sampler.g.num_edges(),)).to(device)\n",
        "\n",
        "  N = 0\n",
        "  sampled_nodes = 0\n",
        "  for num_nodes, subgraphs_nids, subgraphs_eids in loader:\n",
        "    sampled_nodes += num_nodes\n",
        "    sampled_nodes_idx, _node_counts = torch.unique(\n",
        "        torch.cat(subgraphs_nids), return_counts=True)\n",
        "    node_counter[sampled_nodes_idx] += _node_counts\n",
        "\n",
        "    sampled_edges_idx, _edge_counts = torch.unique(\n",
        "        torch.cat(subgraphs_eids), return_counts=True)\n",
        "    edge_counter[sampled_edges_idx] += _edge_counts\n",
        "    N += len(subgraphs_nids)\n",
        "\n",
        "    if sampled_nodes > num_samples * sampler.g.num_nodes():\n",
        "      break\n",
        "    \n",
        "  return compute_norm(sampler.g, node_counter, edge_counter, N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwb2soStNmMX"
      },
      "source": [
        "sampler = NodeSampler(graph, 100)\n",
        "aggr, loss = precompute_norm(sampler, 100, 512, 0) # num_workers = 0 due to CUDA inference\n",
        "\n",
        "assert round(aggr[1].item(), 3) == 0.014\n",
        "assert round(loss[1].item(), 2) == 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGyjwOFwpmvn"
      },
      "source": [
        "### Put all together\n",
        "\n",
        "In this task we will train the GCN using the GraphSAINT sampling approach.\n",
        "\n",
        "Firstly, we need to define the network. It will be vanilla GCN. However, we want to apply the edge and loss normalizations. So the forward function should pass the `edge_weight` to the layer inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HAHiyL-T64O"
      },
      "source": [
        "from dgl.nn import GraphConv\n",
        "\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, hid_dim=8, n_layers=3, dropout_rate=0.1):\n",
        "    super(GCN, self).__init__()\n",
        "    self.layers = nn.ModuleList()\n",
        "    self.layers.append(GraphConv(in_dim, hid_dim, norm=\"right\", activation=F.relu))\n",
        "    for i in range(n_layers - 2):\n",
        "      self.layers.append(GraphConv(hid_dim, hid_dim, norm=\"right\", activation=F.relu))\n",
        "    self.layers.append(GraphConv(hid_dim, out_dim, norm=\"right\"))\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "  \n",
        "  def forward(self, graph, use_norm=True):\n",
        "    h = graph.ndata[\"feat\"]\n",
        "    for layer in self.layers[:-1]:\n",
        "      h = self.dropout(h)\n",
        "      h = layer(graph, h, edge_weight=graph.edata[\"aggr_norm\"] if use_norm else None)\n",
        "    return self.layers[-1](graph, h, edge_weight=graph.edata[\"aggr_norm\"] if use_norm else None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVmWozJKVvLx"
      },
      "source": [
        "Now let us define all requirement for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "samyltrkW1Lu"
      },
      "source": [
        "def collate(batch):\n",
        "  return batch[0]\n",
        "\n",
        "graph = graph.add_self_loop()\n",
        "sampler = NodeSampler(graph, 512)\n",
        "sampler.g.edata[\"aggr_norm\"], sampler.g.ndata[\"loss_norm\"] = \\\n",
        "  precompute_norm(sampler, 1000, 512, 0)\n",
        "sampler.g = sampler.g.to(device)\n",
        "loader = DataLoader(sampler, batch_size=1, collate_fn=collate)\n",
        "model = GCN(graph.ndata['feat'].size(1), label.max().item() + 1)\n",
        "model = model.to(device)\n",
        "evaluator = Evaluator(d_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_qRFam7nuE5"
      },
      "source": [
        "The one epoch trainer is usual. However, we need to apply the loss normalization technique:\n",
        "\n",
        "1. Calculate loss for each point (`reduction=\"none\"`)\n",
        "2. Multiply loss by normalization\n",
        "3. Reduce loss with mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYI6ft3TWxAz"
      },
      "source": [
        "def train(model, loader, optimizer):  \n",
        "  model.train()\n",
        "  for subgraph in tqdm(loader):\n",
        "    subgraph = subgraph.to(device)\n",
        "    subgraph = dgl.add_self_loop(subgraph)\n",
        "    p = model(subgraph)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = F.cross_entropy(\n",
        "        p.to(torch.float32)[subgraph.ndata['is_train']], \n",
        "        subgraph.ndata['label'][subgraph.ndata['is_train']], \n",
        "        reduction=\"none\")\n",
        "    loss = (loss * subgraph.ndata[\"loss_norm\"][subgraph.ndata['is_train']]).mean()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "\n",
        "\n",
        "def evaluate(model, g, labels, mask):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    logits = model(g, use_norm=False)\n",
        "    logits = logits[mask].argmax(dim=1, keepdims=True)\n",
        "    labels = labels[mask]\n",
        "    return evaluator.eval({\n",
        "        'y_true': labels,\n",
        "        'y_pred': logits\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuAJ9vmloj0D"
      },
      "source": [
        "Now let us run the pipeline and see our results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQZZXDxduueu"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import os\n",
        "log_dir = \"./\"\n",
        "\n",
        "def run(model, loader):\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "  scheduler = ReduceLROnPlateau(optimizer, factor=0.9)\n",
        "  best_acc = -1\n",
        "\n",
        "  scores = {\n",
        "        \"train\": [],\n",
        "        \"validation\": [],\n",
        "        \"test\": [],\n",
        "    }\n",
        "  for epoch in range(500):\n",
        "    train(model, loader, optimizer)\n",
        "\n",
        "    model.eval()\n",
        "    if epoch % 10 == 0:\n",
        "      clear_output()\n",
        "      scores['train'].append(evaluate(model, graph, label, train_idx)[\"acc\"])\n",
        "      scores['validation'].append(evaluate(model, graph, label, valid_idx)[\"acc\"])\n",
        "      scores['test'].append(evaluate(model, graph, label, test_idx)[\"acc\"])\n",
        "\n",
        "      plt.title(\"Score dynamics. train: {:.4f}, validation: {:.4f}, test: {:.4f}\".format(\n",
        "          scores['train'][-1], scores['validation'][-1], scores['test'][-1]))\n",
        "      plt.plot(scores['train'], label=\"train\")\n",
        "      plt.plot(scores['validation'], label=\"validation\")\n",
        "      plt.plot(scores['test'], label=\"test\")\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "      \n",
        "      if scores['validation'][-1] > best_acc:\n",
        "        best_acc = scores['validation'][-1]\n",
        "        print('new best val score:', best_acc)\n",
        "        torch.save(model.state_dict(), os.path.join(\n",
        "            log_dir, 'best_model.pkl'))\n",
        "      scheduler.step(scores['validation'][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B090-q55_TLK"
      },
      "source": [
        "run(model, loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s5dePtjhxiN8"
      },
      "source": [
        "sampler = EdgeSampler(g=graph, num_edges=256)\n",
        "sampler.g.edata[\"aggr_norm\"], sampler.g.ndata[\"loss_norm\"] = precompute_norm(sampler, 1000, 512, 0)\n",
        "sampler.g.edata[\"aggr_norm\"] = sampler.g.edata[\"aggr_norm\"].to(device)\n",
        "sampler.g.ndata[\"loss_norm\"] = sampler.g.ndata[\"loss_norm\"].to(device)\n",
        "sampler.g = sampler.g.to(device)\n",
        "loader = DataLoader(sampler, batch_size=1, collate_fn=collate)\n",
        "model = GCN(graph.ndata['feat'].size(1), label.max().item() + 1)\n",
        "model = model.to(device)\n",
        "\n",
        "run(model, loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ByIXuQf8JUL"
      },
      "source": [
        "sampler = RandomWalkSampler(num_roots=10000, length=100, g=graph)\n",
        "sampler.g.edata[\"aggr_norm\"], sampler.g.ndata[\"loss_norm\"] = precompute_norm(sampler, 1000, 512, 0)\n",
        "sampler.g = sampler.g.to(device)\n",
        "loader = DataLoader(sampler, batch_size=1, collate_fn=collate, num_workers=8)\n",
        "model = GCN(graph.ndata['feat'].size(1), label.max().item() + 1)\n",
        "model = model.to(device)\n",
        "\n",
        "run(model, loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTWAyMSeAmgx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}