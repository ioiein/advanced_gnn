{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McKSjhF-NyQJ"
   },
   "source": [
    "# Assignment - Recommender Systems via Graph Neural Networks\n",
    "\n",
    "In this task, we will implement the RippleNet model from the [paper](https://arxiv.org/pdf/1803.03467.pdf). The model utilizes information from the knowledge graph to enforce the recommendation performance.\n",
    "\n",
    "![test](https://recbole.io/docs/v0.1.2/_images/ripplenet.jpg)\n",
    "\n",
    "The picture depicts the general pipeline of the model.\n",
    "\n",
    "The upper part of the image shows the construction of the ripple sets. The seed nodes are the items that users already like. We construct ripple sets as a k-hop neighborhood of the corresponding seed nodes.\n",
    "\n",
    "The architecture scheme defines the prediction pipeline. We iteratively aggregate the information from k-hop neighbors to receive the user embedding. Finally, we take the scalar product between received user embedding and initial item one to receive the probability.\n",
    "\n",
    "We will start by preparing the dataset and visualization of the knowledge graph slice. Then we will implement the torch dataset, which is able to find ripple sets. After, we will define the model, train it and try to interpret some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmoSEvy8IIOO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwmIiYc6IIOT"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EcW6tUhIIOU"
   },
   "source": [
    "We will work with the MovieLens-1m dataset linked to the knowledge base.\n",
    "\n",
    "We will only use the information about user-movie interactions from the MovieLens. The data in the proposed archive is already preprocessed. Firstly, item indices are aligned between the knowledge graph and the rating matrix. Secondly, ratings are converted to binary format. In this task, we will suppose that if the rating is greater than 3, the user likes the movie, so that we will plot the edge between them. Otherwise, the rating is negative, and the edge will be skipped, but we will propagate the negative value to the loss.\n",
    "\n",
    "Let us download the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZlQqbcYSIZAN",
    "outputId": "92c32849-2386-47e0-ba23-99c87f520ef7"
   },
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/netspractice/advanced_gnn/main/assignment_recsys/data/id2relation.json\n",
    "! wget https://raw.githubusercontent.com/netspractice/advanced_gnn/main/assignment_recsys/data/id2title.json\n",
    "! wget https://github.com/netspractice/advanced_gnn/raw/main/assignment_recsys/data/kg_final.npy\n",
    "! wget https://github.com/netspractice/advanced_gnn/raw/main/assignment_recsys/data/ratings_final.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALrTaXnMQYj4"
   },
   "source": [
    "The `ml-kg-preprocessed/ratings_final.npy` contains preprocessed user-item interaction matrix. The `ml-kg-preprocessed/kg_final.npy` contains knowledge graph triplets (head, relation and tail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrJVhZRmIIOV"
   },
   "outputs": [],
   "source": [
    "ratings = np.load(\"ratings_final.npy\")\n",
    "kg_edges = np.load(\"kg_final.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZnjDMlkQo6I"
   },
   "source": [
    "Additionally, we have to mappings: movie id to title and relation id to its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jkF2oAbJQzul"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "\n",
    "with open(\"id2relation.json\") as f:\n",
    "    id2relation = {int(i):j for i, j in json.load(f).items()}\n",
    "\n",
    "with open(\"id2title.json\") as f:\n",
    "    id2title = {int(i):j for i, j in json.load(f).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCFDT_FAV01b"
   },
   "source": [
    "And for the visualization let us convert kg triplets to the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOW0V1ErV59d"
   },
   "outputs": [],
   "source": [
    "kg_edges_pd = pd.DataFrame(kg_edges)\n",
    "kg_edges_pd.columns = [\"head\", \"relation\", \"tail\"]\n",
    "kg_edges_pd.relation = kg_edges_pd.relation.map(id2relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7NamJ7sM9xi"
   },
   "source": [
    "## Task 1. Knowledge subgraph visualization (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiDs_PhCVDA0"
   },
   "source": [
    "Let us visualize a part of a graph to understand how it looks like.\n",
    "\n",
    "Firstly, we need to define the function `find_neighbors_by_tail` that will return all triplets adjacent to tails achievable from the specific `movie_id`. Due to the large degree of specific entities (e.g. English language), we will truncate the number of possible movie_id tails by the least `k_tails`. The `kg_edges` \n",
    "\n",
    "The function should work as follows:\n",
    "\n",
    "1. Select the part of `kg_edges_pd` where `head` is equal to `movie_id`\n",
    "2. Extract all possible tails\n",
    "3. Select the part of `kg_edges_pd` where `tail` is in tails array from step 2\n",
    "4. Calculate the occurrence of tails in the subgraph 3\n",
    "5. Select only the least `k_tails` from them\n",
    "6. Filter the subgraph from 3 by the tails list from 5\n",
    "7. Filter the subgraph whether `head` is in the `id2title` map\n",
    "8. Create column `tail_type` with the last word after the dot in the `relation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "FfBFeXPPVDlN",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08b7eb960a48b654e5d1f4ac2496194d",
     "grade": false,
     "grade_id": "cell-ad1041767c7b9d62",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_neighbors_by_tail(movie_id, kg_edges_pd, k_tails=5):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1ZiB-21BXaIh",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5434b4ecfb971fc21a9bd18ab9966d0a",
     "grade": true,
     "grade_id": "cell-5cf90766a9e97754",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "neighbors = find_neighbors_by_tail(0, kg_edges_pd)\n",
    "assert neighbors.columns.tolist() == [\"head\", \"relation\", \"tail\", \"tail_type\"]\n",
    "assert neighbors.shape[0] == 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZLoA1vbX8u1"
   },
   "source": [
    "Next, let us define the `find_positions` method, which takes the result of `find_neighbors_by_tail` and assign the positions on a 2d figure for each entity. Also, it should return the mapping from entity id to its types. The type of the tail is defined by its `tail_type` columns values. The type of elements in the `head` column is a `\"title\"`.\n",
    "\n",
    "The `find_positions_and_ntypes` should work as follows:\n",
    "\n",
    "1. Define mapping from entity id to its type as described in the previous paragraph\n",
    "2. If the node type is equal to the `title`, then x coordinate equals `1`. Otherwise, it equals `2`\n",
    "3. The elements inside each group should be placed through equal intervals inside the `(0, 1)` range\n",
    "4. Method should return two dictionaries. The first with a mapping from entity id to the tuple of (x, y) position, the second with the mapping from entity id to the node types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "nrYvqT1mi4zj",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "953817045d52fdc7a8f1a7ecf3016f6e",
     "grade": false,
     "grade_id": "cell-6a8a42aa4a93c920",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_positions_and_ntypes(neighbors):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "GnyaUincb23U",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4548963cbff8aa0b55ecef1e92720ef8",
     "grade": true,
     "grade_id": "cell-234b70d27e6efc8c",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pos, ntype = find_positions_and_ntypes(neighbors)\n",
    "assert ntype[0] == \"title\"\n",
    "assert pos[0][0] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hA4iPwVGZCC1"
   },
   "source": [
    "Now, we can visualize the knowledge graph part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "NygeaBMti4wd",
    "outputId": "00304671-6b14-468a-a5d5-dfcdfb11c498"
   },
   "outputs": [],
   "source": [
    "def visualize_kg_part(movie_id, kg_edges, k_tails=5):\n",
    "    neighbors = find_neighbors_by_tail(movie_id, kg_edges, k_tails)\n",
    "    pos, ntypes = find_positions_and_ntypes(neighbors)\n",
    "    \n",
    "    g = nx.from_edgelist(neighbors[[\"head\", \"tail\"]].values)\n",
    "    labels = ntypes.copy()\n",
    "    labels.update({i: j for i,j in id2title.items() if i in ntypes})\n",
    "    f, ax = plt.subplots(1, 1, figsize=[15, 10])\n",
    "    plt.axis(\"off\")\n",
    "    ax.set_xlim((0.5, 2.5))\n",
    "    nx.draw_networkx_labels(g, pos=pos, labels=labels)\n",
    "    nx.draw_networkx_nodes(g, pos=pos,  ax=ax, node_size=[(100 if i == movie_id else 5) for i in g.nodes])\n",
    "    nx.draw_networkx_edges(g, pos=pos, edgelist=g.edges(), alpha=0.3)\n",
    "\n",
    "visualize_kg_part(0, kg_edges_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SG5RdtsYapei"
   },
   "source": [
    "Here, we can see that `Toy Story` has similar director and writers with the second part of the title and with `Bug's Life`. Additionally, it shares writers with the movies `Titan A.E.`, `Alien: Resurrection` and `Buffy the Vampire Slayer`.\n",
    "\n",
    "We can see similar logic in the picture from the paper.\n",
    "\n",
    "<img src=\"https://img2018.cnblogs.com/i-beta/1338991/201911/\n",
    "1338991-20191128135255625-1310374474.png\" width=\"500\" />\n",
    "\n",
    "The RippleNet model intuition is that knowledge graph defines meaningful connections between movie titles within the two hops of a knowledge graph. So this information can help to find the best choices to recommend.\n",
    "\n",
    "For example, let us consider the case of collaborative filtering. It defines the similarity between titles through the item co-occurrence within the history of a user. So, for example, in this case `Toy Story` and `Toy Story 2` will be similar only when many users watch both films. However, a knowledge graph provides an opportunity to know its similarity in advance because it shares several neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7X2VnZvIIOW"
   },
   "source": [
    "Now, we need to split ratings into train, validation and test parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwhtnuMhIIOX"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "idx = np.random.permutation(ratings.shape[0])\n",
    "\n",
    "train_idx = idx[:int(0.6 * idx.shape[0])]\n",
    "val_idx = idx[int(0.6 * idx.shape[0]):int(0.8 * idx.shape[0])]\n",
    "test_idx = idx[int(0.8 * idx.shape[0]):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxzXuAOmUtGH"
   },
   "source": [
    "## Task 2. Graphs preparation (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyMvDVMycVT3"
   },
   "source": [
    "Let us define the train graph of user-item interactions. It could be represented as a mapping from the user id to the list of all liked (positive) titles by a user.\n",
    "\n",
    "The rating array has three columns:\n",
    "1. User id\n",
    "2. Item id\n",
    "3. Rating (0 or 1)\n",
    "\n",
    "The function should work as follows:\n",
    "\n",
    "1. Iterate over `train_idx`\n",
    "2. If `rating` is positive, then append item id to corresponding user id key history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "v7zYzhT8duj7",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc78d128d3a3d2d4d0ff2c1f4367ae58",
     "grade": false,
     "grade_id": "cell-09a888172177b492",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_ratings_to_user_history(ratings, train_idx):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return user_history_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Rwfus0God8Qt",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75ba2c57c399d8b506e4e62faaeca5ab",
     "grade": true,
     "grade_id": "cell-12f05d9cf2426e4f",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "user_history_dict = convert_ratings_to_user_history(ratings, train_idx)\n",
    "assert 624 in user_history_dict[0]\n",
    "assert len(user_history_dict[0]) == 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvfVrRYnIIOa"
   },
   "source": [
    "Let us filter indices if users are not presented in the user history graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLYCR9pLIIOa"
   },
   "outputs": [],
   "source": [
    "train_idx = [i for i in train_idx if ratings[i][0] in user_history_dict]\n",
    "val_idx = [i for i in val_idx if ratings[i][0] in user_history_dict]\n",
    "test_idx = [i for i in test_idx if ratings[i][0] in user_history_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49Fx0zb8IIOb"
   },
   "source": [
    "And finally, we can define the train, val and test \n",
    "subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wemgQeLLIIOb"
   },
   "outputs": [],
   "source": [
    "train_data = ratings[train_idx]\n",
    "val_data = ratings[val_idx]\n",
    "test_data = ratings[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzyv0IfpIIOc"
   },
   "source": [
    "Now, we need to transform our knowledge graph edges into the dictionary where the key is heads and values is a list of tuples `(tail, relation)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "Ez8dOeSGIIOc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0498ce555065b51bcf7a194f8f653773",
     "grade": false,
     "grade_id": "cell-768d469fb230a058",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def transform_kg(kg_edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "z3Ly-Y7sfCQP",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "783d34c2b7f4cf739790bbde97a0aa03",
     "grade": true,
     "grade_id": "cell-90cf6b02c0ba8404",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "kg = transform_kg(kg_edges)\n",
    "\n",
    "assert kg[0][0] == (2755, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61blRsOIfLm8"
   },
   "source": [
    "## Task 3. Ripple sets (3 points)\n",
    "\n",
    "One of the main concepts in the paper is a RippleSet. RippleSet is an array of all neighboring triplets to our node within the given number of hop.\n",
    "\n",
    "Some entities have a substantial degree that could cover almost the whole graph. So to reduce the complexity of a model, we will use only a fixed uniform subsample of neighbors on each hop.\n",
    "\n",
    "Here, the `find_ripple` method should be defined in the following way\n",
    "1. Iterate over the number of hops\n",
    "2. Find neighbors to the tails of the previous hop (at the start, it is equal to the user history array)\n",
    "3. Preserve the triplet defined by it (head, relation, tail) in different arrays. \n",
    "4. Convert it to the torch.LongTensor\n",
    "5. Append it to the user ripple set array.\n",
    "\n",
    "So ripple set for a user with two- and three-hop neighbors sample will look like\n",
    "\n",
    "```\n",
    "[\n",
    "  [\n",
    "    torch.LongTensor([0, 1, 2]), # first hop head ids\n",
    "    torch.LongTensor([0, 1, 2]), # first hop relation ids\n",
    "    torch.LongTensor([5, 2, 9]), # first hop tail ids\n",
    "  ],\n",
    "  [\n",
    "    torch.LongTensor([5, 2, 2]), # second hop head ids (subsample of previous tails)\n",
    "    torch.LongTensor([3, 1, 5]), # second hop head ids\n",
    "    torch.LongTensor([10, 3, 1]), # second hop relation ids\n",
    "  ]\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "TrzpoUsQjXEM",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35296b0cb14e93e53a55473707163d08",
     "grade": false,
     "grade_id": "cell-d9fd4c3a51ca6f6f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_ripple(user, user_history_dict, kg, num_hops=2, num_neighbors=32):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "w_Sovp9WjpVr",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de54d27bff52a268bc522109fa10d163",
     "grade": true,
     "grade_id": "cell-f64f0c0929409e4f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ripple = find_ripple(0, user_history_dict, kg)\n",
    "\n",
    "assert len(ripple) == 2\n",
    "assert ripple[0][0].shape[0] == 32\n",
    "assert len(set([i.item() for i in ripple[0][0]]) - set(user_history_dict[0])) == 0\n",
    "assert len(set([i.item() for i in ripple[1][0]]) - set([i.item() for i in ripple[0][2]])) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1xlJPaWknNM"
   },
   "source": [
    "In the next cell, we will implement the `__getitem__` method for RippleDataset.\n",
    "\n",
    "The `idx` is an integer index of an element in the rating matrix.\n",
    "We need to extract the user, item and label from it and add the ripple set for a given user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "RqhA97YeIIOd",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51f96ef6d10f5fa9ce41dfeac3effa79",
     "grade": false,
     "grade_id": "cell-357689f65e86207b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class RippleDataset(Dataset):\n",
    "    def __init__(self, ratings, ripple_set):\n",
    "        self.ratings = torch.LongTensor(ratings)\n",
    "        self.ripple_set = ripple_set\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.ratings.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return user, item, label, ripple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdrTu4aZkdZz"
   },
   "source": [
    "Now, we can define the dataloader for train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sHY1O8YIIOe"
   },
   "outputs": [],
   "source": [
    "ripple_set = {user: find_ripple(user, user_history_dict, kg) for user in user_history_dict}\n",
    "\n",
    "train_dataset = RippleDataset(train_data, ripple_set)\n",
    "val_dataset = RippleDataset(val_data, ripple_set)\n",
    "test_dataset = RippleDataset(test_data, ripple_set)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SxZ6q9VblF5M",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "529f9e533833295ae36edcdc0b45a94a",
     "grade": true,
     "grade_id": "cell-d698c636aa1d38b0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "user, item, label, ripple = train_dataset.__getitem__(0)\n",
    "assert user.item() == 441\n",
    "assert item.item() == 658\n",
    "assert label.item() == 1\n",
    "assert len(ripple) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZhpiTJIIIOh"
   },
   "source": [
    "## Task 4. Model definition (2 points)\n",
    "\n",
    "To explain how the model works, let us refer to the picture from the start of the notebook.\n",
    "\n",
    "![test](https://recbole.io/docs/v0.1.2/_images/ripplenet.jpg)\n",
    "\n",
    "1. Firstly, we need to encode all the entities: items, heads, relations and tails in the ripple sets\n",
    "2. Then, we need to construct vectors that represent each hop of neighborhood (`key_addressing` method)\n",
    "3. We need to calculate the user embedding by taking the sum of all hop vectors.\n",
    "4. Finally, we can calculate the affinity score between user and item representations by taking the dot product of them and normalizing it with a sigmoid function\n",
    "\n",
    "\n",
    "The `key_addressing` function works as follows:\n",
    "\n",
    "1. Iterate over each hop\n",
    "2. Project the head to the tail space by multiplying it on the relations embedding\n",
    "3. Multiply the item embeddings by the projected heads from step 2 and normalize it via softmax to calculate the tail weights\n",
    "4. Calculate the hop representation by taking the weighted average of tail vectors relative to probabilities from step 3.\n",
    "5. Concatenate the item embeddings with a result of step 4.\n",
    "6. Project the result of 5 using `transform_matrix`\n",
    "7. Safe the resulting hop vector\n",
    "8. After all iterations, sum all weights from 7 to receive the user_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "NtNyyLbNIIOh",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a216d37b4aa02473f4fdfb5d57cd52e",
     "grade": false,
     "grade_id": "cell-8245d9d501161f21",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class RippleNet(nn.Module):\n",
    "    def __init__(self, n_entity, n_relation, dim):\n",
    "        super(RippleNet, self).__init__()\n",
    "\n",
    "        self.n_entity = n_entity\n",
    "        self.n_relation = n_relation\n",
    "        self.dim = dim\n",
    "\n",
    "        self.entity_emb = nn.Embedding(self.n_entity, self.dim)\n",
    "        self.relation_emb = nn.Embedding(self.n_relation, self.dim * self.dim)\n",
    "        self.transform_matrix = nn.Linear(self.dim * 2, self.dim, bias=False)\n",
    "\n",
    "    def forward(self, items, ripples):\n",
    "        item_embeddings = self.entity_emb(items)\n",
    "        head_embs = []\n",
    "        rel_embs = []\n",
    "        tail_embs = []\n",
    "        \n",
    "        for heads, rels, tails in ripples:\n",
    "            head_embs.append(self.entity_emb(heads))\n",
    "            tail_embs.append(self.entity_emb(tails))\n",
    "            rel_embs.append(self.relation_emb(rels).reshape(-1, rels.shape[1], self.dim, self.dim))\n",
    "\n",
    "        user_embedding, item_embeddings = self.key_addressing(\n",
    "            head_embs, tail_embs, rel_embs, item_embeddings\n",
    "        )\n",
    "\n",
    "        affinity = (user_embedding * item_embeddings).sum(1).sigmoid()\n",
    "        return affinity, (head_embs, rel_embs, tail_embs)\n",
    "\n",
    "    def key_addressing(self, head_embs, tail_embs, rel_embs, item_embeddings):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_BIFpVa9aXR"
   },
   "outputs": [],
   "source": [
    "for (user, item, label, ripple) in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9GU7-85vIIOi",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2fa14d1d45d9dfdb8e4685bfc3b049b",
     "grade": true,
     "grade_id": "cell-b0eaecaf95dbb65a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "n_ent = len(set(kg_edges[:, 0]) | set(kg_edges[:, 2]))\n",
    "n_rel = len(set(kg_edges[:, 1]))\n",
    "\n",
    "model = RippleNet(n_ent, n_rel, dim=8)\n",
    "scores, embs = model(item, ripple)\n",
    "\n",
    "assert scores[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ie7gvuwnIIOi"
   },
   "source": [
    "## Task 5. Loss definition (2 points)\n",
    "\n",
    "In this part, we will define the loss function `compute_losses`.\n",
    "It consists of three parts: binary cross-entropy (BCE) loss between item labels and score, knowledge graph embedding (KGE) loss and knowledge graph l2 regularization.\n",
    "\n",
    "BCE loss could be calculated via the default function from PyTorch.\n",
    "\n",
    "The KGE part of loss aims to learn the fine-grained entity and relation representations. It equals the dot product between head to tail projection (h * r) and tail vectors normalized by sigmoid. After calculating probability, we need to take the mean over all samples.\n",
    "\n",
    "The l2 regularization is a sum over the L2-norm for each head, relation and tail matrices. It is better to divide it by 3 to take the mean between matrices and normalize by the overall number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcPIgYnNIIOi"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "GOLELRtcIIOj",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f7feb4f1b15371fb4e612604ee81f59",
     "grade": false,
     "grade_id": "cell-686efc1fb10360a6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_losses(scores, labels, heads, relations, tails):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return bce_loss, kge_loss, l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DMQ2Z_e9Kucj",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5309865325ff2ffda1cd74d870666cdc",
     "grade": true,
     "grade_id": "cell-01a031cca82b88a8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "bce, kge, l2 = compute_losses(torch.FloatTensor([0.5]), torch.FloatTensor([1]),\n",
    "                              [torch.ones(10, 32, 8), torch.ones(10, 32, 8)],\n",
    "                              [torch.ones(10, 32, 8, 8), torch.ones(10, 32, 8, 8)],\n",
    "                              [torch.ones(10, 32, 8), torch.ones(10, 32, 8)],)\n",
    "\n",
    "assert round(bce.item(), 4) == 0.6931\n",
    "assert round(kge.item()) == 2\n",
    "assert round(l2.item()) == 853"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPnAFexgIIOj"
   },
   "source": [
    "## Task 5. Put all together (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gw-KDj_dIIOj"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def train(model, opt, loader, kge_weight=0.01, l2_weight=1e-4):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for users, items, labels, ripples in tqdm(loader):\n",
    "        scores, embs = model(items.to(device), [[i.to(device) for i in ripple] for ripple in ripples])\n",
    "        bce_loss, kge_loss, l2_loss = compute_losses(scores, labels.to(device), *embs)\n",
    "        loss = bce_loss + kge_weight * kge_loss + l2_weight * l2_loss\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "def evaluate(model, loader, kge_weight=0.01, l2_weight=1e-4):\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    losses = []\n",
    "    for users, items, labels, ripples in tqdm(loader):\n",
    "        scores, embs = model(items.to(device), [[i.to(device) for i in ripple] for ripple in ripples])\n",
    "        bce_loss, kge_loss, l2_loss = compute_losses(scores, labels.to(device), *embs)\n",
    "        losses.append((bce_loss + kge_weight * kge_loss + l2_weight * l2_loss).item())\n",
    "        all_scores.append(scores.detach().cpu().numpy())\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "    all_scores = np.concatenate(all_scores)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    ap = average_precision_score(all_labels, all_scores)\n",
    "    return ap, np.mean(losses)\n",
    "\n",
    "def run(model, opt, train_loader, val_loader, test_loader, kge_weight=0.01, l2_weight=1e-4, n_epoch=10):\n",
    "    train_loss_log = []\n",
    "    val_loss_log = []\n",
    "    val_ap_log = []\n",
    "    for epoch in range(n_epoch):\n",
    "        train_loss_log.append(train(model, opt, train_loader, kge_weight, l2_weight))\n",
    "        val_ap, val_loss = evaluate(model, val_loader, kge_weight=0.01, l2_weight=1e-4)\n",
    "        val_loss_log.append(val_loss)\n",
    "        val_ap_log.append(val_ap)\n",
    "        clear_output()\n",
    "        f, ax = plt.subplots(1, 2, figsize=[13, 4])\n",
    "        ax[0].plot(train_loss_log, label=f\"train: {train_loss_log[-1]:.2f}\")\n",
    "        ax[0].plot(val_loss_log, label=f\"val: {val_loss_log[-1]:.2f}\", c=\"C1\")\n",
    "        ax[1].plot(val_ap_log)\n",
    "        f.suptitle(f\"Best score: {max(val_ap_log):.2f} at epoch: {np.argmax(val_ap_log):.2f}, last score: {val_ap_log[-1]:.2f} at epoch {epoch}\")\n",
    "        f.show()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    ap, loss = evaluate(model, test_loader, kge_weight=0.01, l2_weight=1e-4)\n",
    "    print(f\"Test score: {ap:.2f}, test loss: {loss:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "CtH1Q_qWIIOk",
    "outputId": "9082b46c-9780-4d3b-e504-609bd324f931"
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = RippleNet(n_ent, n_rel, dim=8)\n",
    "model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), 3e-4)\n",
    "\n",
    "# run(model, opt, test_loader, test_loader, test_loader, n_epoch=22)\n",
    "\n",
    "# with open(\"weights.pth\", \"wb\") as f:\n",
    "#     torch.save(model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koMGdTPOAldl"
   },
   "source": [
    "Download your weights here and load to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "sbsiHolIhlDB",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "506ff1acc963356b1b3aa0922d9c841d",
     "grade": false,
     "grade_id": "cell-6bee073cff2bcebd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "1bf62dbd-50d9-4af8-abc6-5967d2cf1477"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "# model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OlJWemyPC-rc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6d8146928e6a4106aabd700007ee9bc",
     "grade": true,
     "grade_id": "cell-34a5ab2f241065f5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scores, embs = model(item.to(device), [[j.to(device) for j in i] for i in ripple])\n",
    "\n",
    "labels = label.to(device)\n",
    "bce, kge, l2 = compute_losses(scores, labels, *embs)\n",
    "assert bce.item() < 0.6 and bce.item() > 0.3\n",
    "assert kge.item() < 1 and kge.item() > 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yPnKGSqIIOk"
   },
   "source": [
    "## Prediction example\n",
    "\n",
    "Let us visualize some model predictions to understand how it works.\n",
    "\n",
    "We will take the sample user with id 3084.\n",
    "\n",
    "We want to predict scores for previously unseen items, so we select all unique items from ratings and remove the elements from user history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eS-J7uPNIIOk"
   },
   "outputs": [],
   "source": [
    "user = 3084\n",
    "items = torch.LongTensor(list(set(np.unique(ratings[:, 1])) - set(user_history_dict[user])))\n",
    "ripples = train_dataset.ripple_set[user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4fHayO-BE5o"
   },
   "source": [
    "Now, we can calculate scores and take the top five items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "oM-Nx7SjIIOk",
    "outputId": "476fc47c-b691-4980-d7f4-e950ef2422c2"
   },
   "outputs": [],
   "source": [
    "scores, embs = model(items.to(device), [[i.reshape(1, -1).to(device) for i in ripple] for ripple in ripples])\n",
    "\n",
    "top5 = items[scores.argsort()].numpy()[-5:]\n",
    "top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qj4ZnF9lBTUM"
   },
   "source": [
    "Let us check is there any intersection of our prediction and ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yL0VoRxsieqw",
    "outputId": "38cf5694-ed45-41d3-ba71-30a7a38293e3"
   },
   "outputs": [],
   "source": [
    "set(top5) & set(test_data[test_data[:, 0] == user][:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU04p-BYBZ8V"
   },
   "source": [
    "Finally, we can visualize a subsample of the knowledge graph defined by user history and item 341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "l2gT4OjwieoF",
    "outputId": "ffb5cbcf-2dda-4d4b-ac60-7a1e0662480a"
   },
   "outputs": [],
   "source": [
    "tmp = kg_edges_pd[kg_edges_pd[\"head\"].isin(user_history_dict[user] + [1554])]\n",
    "visualize_kg_part(1554, tmp[~(tmp.relation.str.contains(\"language\")|tmp.relation.str.contains(\"country\") |tmp.relation.str.contains(\"rating\"))], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55_98k7Zyelg"
   },
   "source": [
    "We can see that given user watch many films of similar genres\n",
    "\n",
    "Let us check another item that is wrong but appears within the top five predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "Bb9NJZAxiemT",
    "outputId": "0f5f5c0d-7f73-485d-e453-44e3fe9816de"
   },
   "outputs": [],
   "source": [
    "tmp = kg_edges_pd[kg_edges_pd[\"head\"].isin(user_history_dict[user] + [740])]\n",
    "visualize_kg_part(740, tmp[~(tmp.relation.str.contains(\"language\")|tmp.relation.str.contains(\"country\") |tmp.relation.str.contains(\"rating\"))], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYALlXYhyoXP"
   },
   "source": [
    "Results for this prediction is similar. The user watches several films from the creators of Raiders of the Lost Ark and many films of similar genres. Moreover, this film is a part of an Indiana Jones series. The given user watched Indiana Jones and the Last Crusade previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5Axm5Niiefn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Made 2021. Advanced GNN. HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
